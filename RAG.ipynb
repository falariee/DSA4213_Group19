{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b99838b",
   "metadata": {},
   "source": [
    "## RAG PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd96f9",
   "metadata": {},
   "source": [
    "### Set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "BioGPT model loaded for embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x00000278DB54D300>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jasper\\anaconda3\\envs\\biogpt_gpu\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Jasper\\anaconda3\\envs\\biogpt_gpu\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openai import AzureOpenAI\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from bert_score import score as bert_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "from ragas import evaluate\n",
    "\n",
    "AZURE_OPENAI_API_KEY = \"ae9587f69088409992009cb7bcf61436\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://genai-openai-eus.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"genai-GPT4o-mini\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_NAME = \"microsoft/biogpt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "print(\"BioGPT model loaded for embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26970c85",
   "metadata": {},
   "source": [
    "### Build Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa18f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: 211269 rows\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"PubMedQA_artificial_RAG.csv\")\n",
    "train_df = train_df.dropna(subset=[\"context\", \"question\", \"long_answer\"])\n",
    "print(f\"Loaded training data: {len(train_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4264721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding contexts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing batches: 100%|██████████| 413/413 [1:10:08<00:00, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 211269 documents into ChromaDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"pubmedqa_biogpt\"\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "except:\n",
    "    pass\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "context_texts = train_df[\"context\"].astype(str).tolist()\n",
    "print(\"Embedding contexts...\")\n",
    "\n",
    "for i in tqdm(range(0, len(context_texts), 512), desc=\"Indexing batches\"):\n",
    "    batch = context_texts[i:i+512]\n",
    "    emb = embed_texts_biogpt(batch)\n",
    "    ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
    "    collection.add(documents=batch, embeddings=emb.tolist(), ids=ids)\n",
    "\n",
    "print(f\"Indexed {len(context_texts)} documents into ChromaDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da770f6e",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7076d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts_biogpt(texts, batch_size=8):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(device)\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            emb = outputs.last_hidden_state.mean(dim=1).cpu()\n",
    "            embeddings.append(emb)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b15df9",
   "metadata": {},
   "source": [
    "### Retrieval from Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391a7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=5):\n",
    "    q_emb = embed_texts_biogpt([question])[0].tolist()\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    retrieved_docs = results[\"documents\"][0]\n",
    "    return \"\\n\\n\".join(retrieved_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27f9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    prompt = f\"\"\"You are a biomedical assistant. \n",
    "Use the context below to answer concisely.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccb2a8",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92367f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 1000/1000 [34:22<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using 1000 valid samples for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")[\"train\"]\n",
    "questions = dataset[\"question\"][:1000]\n",
    "gold_answers = dataset[\"long_answer\"][:1000]\n",
    "\n",
    "pred_answers, contexts = [], []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Generating answers\"):\n",
    "    try:\n",
    "        ctx = retrieve_context(q)\n",
    "        ans = generate_answer(q, ctx)\n",
    "        pred_answers.append(ans)\n",
    "        contexts.append(ctx)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped one: {e}\")\n",
    "\n",
    "min_len = min(len(questions), len(pred_answers), len(contexts), len(gold_answers))\n",
    "questions, pred_answers, contexts, gold_answers = [\n",
    "    lst[:min_len] for lst in [questions, pred_answers, contexts, gold_answers]\n",
    "]\n",
    "print(f\" Using {min_len} valid samples for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea17e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 samples for evaluation.\n"
     ]
    }
   ],
   "source": [
    "def sanitize_list(lst):\n",
    "    return [str(x) for x in lst if x not in [None, float(\"nan\"), ...]]\n",
    "\n",
    "min_len = min(len(questions), len(pred_answers), len(contexts), len(gold_answers))\n",
    "questions, pred_answers, contexts, gold_answers = [\n",
    "    sanitize_list(lst[:min_len]) for lst in [questions, pred_answers, contexts, gold_answers]\n",
    "]\n",
    "print(f\"Using {min_len} samples for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4b3ea",
   "metadata": {},
   "source": [
    "### Compute BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10f78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabe8f9cec504747a4589e7350cdd542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653e7aedb658452f96a2f2845a4c4bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.01 seconds, 124.88 sentences/sec\n",
      "Average BERTScore (semantic F1): 0.8692\n",
      "\n",
      "Threshold = 1\n",
      "Accuracy  : 0.0000\n",
      "Precision : 0.0000\n",
      "Recall    : 0.0000\n",
      "F1-score  : 0.0000\n",
      "Saved: RAG_Evaluation_Results.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from bert_score import score as bert_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\nEvaluating BERTScore...\")\n",
    "P, R, F1 = bert_score(pred_answers, gold_answers, lang=\"en\", verbose=True)\n",
    "mean_bert_f1 = F1.mean().item()\n",
    "print(f\"Average BERTScore (semantic F1): {mean_bert_f1:.4f}\")\n",
    "\n",
    "threshold = 1\n",
    "binary_preds = [1 if f >= threshold else 0 for f in F1]\n",
    "binary_labels = [1] * len(binary_preds)\n",
    "\n",
    "accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "precision = precision_score(binary_labels, binary_preds, zero_division=0)\n",
    "recall = recall_score(binary_labels, binary_preds, zero_division=0)\n",
    "f1_cls = f1_score(binary_labels, binary_preds, zero_division=0)\n",
    "\n",
    "print(f\"\\nThreshold = {threshold}\")\n",
    "print(f\"Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall    : {recall:.4f}\")\n",
    "print(f\"F1-score  : {f1_cls:.4f}\")\n",
    "\n",
    "# Save results\n",
    "eval_df = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"context\": contexts,\n",
    "    \"pred_answer\": pred_answers,\n",
    "    \"gold_answer\": gold_answers,\n",
    "    \"bert_f1\": F1.tolist(),\n",
    "    \"is_correct\": binary_preds\n",
    "})\n",
    "eval_df.to_csv(\"RAG_Evaluation_Results.csv\", index=False)\n",
    "eval_df.to_excel(\"RAG_Evaluation_Results.xlsx\", index=False)\n",
    "print(\"Saved: RAG_Evaluation_Results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec07241",
   "metadata": {},
   "source": [
    "### RAGAS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, asyncio, pandas as pd\n",
    "from datasets import load_dataset\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# 1. Azure OpenAI Setup\n",
    "# ============================================================\n",
    "AZURE_OPENAI_API_KEY = \"ae9587f69088409992009cb7bcf61436\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://genai-openai-eus.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"genai-GPT4o-mini\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "async def llm_score(prompt: str) -> float:\n",
    "    \"\"\"LLM judge that returns a normalized score between 0–1.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator who outputs only a number between 0 and 1.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        text = response.choices[0].message.content.strip()\n",
    "        # Extract numeric portion\n",
    "        for token in text.split():\n",
    "            try:\n",
    "                val = float(token)\n",
    "                return min(max(val, 0.0), 1.0)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    return 0.0\n",
    "\n",
    "# ============================================================\n",
    "# 2. Metric Prompts\n",
    "# ============================================================\n",
    "def make_prompts(question, context, answer):\n",
    "    return {\n",
    "        \"faithfulness\": f\"\"\"\n",
    "You are evaluating the factual consistency of an answer given the context.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: {answer}\n",
    "\n",
    "Rate how factually supported the answer is by the context (0 = not supported, 1 = fully supported).\n",
    "Respond with a single number between 0 and 1.\"\"\",\n",
    "\n",
    "        \"answer_relevancy\": f\"\"\"\n",
    "You are evaluating how relevant an answer is to a given question.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Rate how directly and correctly the answer addresses the question (0 = unrelated, 1 = fully relevant).\n",
    "Respond with only a number between 0 and 1.\"\"\",\n",
    "\n",
    "        \"context_precision\": f\"\"\"\n",
    "You are evaluating the relevance of the retrieved context.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Rate how much of the context is relevant to answering the question (0 = mostly irrelevant, 1 = fully relevant).\n",
    "Respond with only a number between 0 and 1.\"\"\",\n",
    "\n",
    "        \"context_recall\": f\"\"\"\n",
    "You are evaluating how completely the context covers the information required to answer the question.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Rate how much of the necessary information is contained in the context (0 = missing key info, 1 = fully complete).\n",
    "Respond with only a number between 0 and 1.\"\"\"\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 3. Load Unseen Dataset\n",
    "# ============================================================\n",
    "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")[\"train\"].select(range(1000))\n",
    "questions = [ex[\"question\"] for ex in dataset]\n",
    "contexts = [\" \".join(ex[\"context\"][\"contexts\"]) for ex in dataset]\n",
    "pred_answers = [\"Generated placeholder answer\" for _ in dataset]  # <-- plug in your model outputs\n",
    "\n",
    "# ============================================================\n",
    "# 4. Evaluate Each Metric via GPT-4o\n",
    "# ============================================================\n",
    "async def evaluate_llm_metrics():\n",
    "    results = []\n",
    "    for i, (q, c, a) in enumerate(zip(questions, contexts, pred_answers), start=1):\n",
    "        print(f\"\\nEvaluating sample {i}/{len(questions)}\")\n",
    "        prompts = make_prompts(q, c, a)\n",
    "        f = await llm_score(prompts[\"faithfulness\"])\n",
    "        r = await llm_score(prompts[\"answer_relevancy\"])\n",
    "        p = await llm_score(prompts[\"context_precision\"])\n",
    "        rec = await llm_score(prompts[\"context_recall\"])\n",
    "\n",
    "        results.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"faithfulness\": f,\n",
    "            \"answer_relevancy\": r,\n",
    "            \"context_precision\": p,\n",
    "            \"context_recall\": rec\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ============================================================\n",
    "# 5. Run Evaluation + Compute Averages\n",
    "# ============================================================\n",
    "async def main():\n",
    "    results = await evaluate_llm_metrics()\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"RAGAS_Eval_Scores.csv\", index=False)\n",
    "    print(\"\\nSaved LLM_Eval_Scores.csv\")\n",
    "\n",
    "    # Compute averages\n",
    "    avg_scores = df[[\"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\"]].mean()\n",
    "    print(\"\\n=== Average LLM Evaluation Scores ===\")\n",
    "    print(avg_scores)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biogpt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
