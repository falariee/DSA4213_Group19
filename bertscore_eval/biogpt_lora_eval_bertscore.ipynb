{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c2d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "   VRAM: 17.1 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from bert_score import score as bert_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cuda\":\n",
    "        _ = torch.zeros(1).to(device)\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU instead.\")\n",
    "except Exception as e:\n",
    "    print(f\"CUDA not supported, defaulting to CPU: {e}\")\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a8239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading base model: microsoft/biogpt\n",
      "Applying PEFT adapter from: ./biogpt_lora_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model loaded successfully on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001B4C0EC39C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jasper\\anaconda3\\envs\\biogpt_gpu\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Jasper\\anaconda3\\envs\\biogpt_gpu\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"microsoft/biogpt\"\n",
    "ADAPTER_PATH = \"./biogpt_lora_finetuned\"\n",
    "\n",
    "print(f\"\\nLoading base model: {BASE_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Fix BioGPT missing pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Attach PEFT adapter\n",
    "print(f\"Applying PEFT adapter from: {ADAPTER_PATH}\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "\n",
    "# Move to device and eval\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"PEFT model loaded successfully on {device}\")\n",
    "THRESHOLD = 0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab919425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")[\"train\"]  # evaluation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f80ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 1000/1000 [25:01<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated_answers = []\n",
    "reference_answers = []\n",
    "\n",
    "for sample in tqdm(dataset, desc=\"Generating answers\"):\n",
    "    question = sample[\"question\"]\n",
    "    context_passages = sample[\"context\"][\"contexts\"]\n",
    "    context = \" \".join(context_passages)\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            if \"no kernel image\" in str(e):\n",
    "                print(\"CUDA kernel mismatch, running on CPU instead.\")\n",
    "                model = model.to(\"cpu\")\n",
    "                device = torch.device(\"cpu\")\n",
    "                outputs = model.generate(**inputs.to(device), max_new_tokens=128, do_sample=False)\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_answer = generated_text.replace(prompt, \"\").strip()\n",
    "    generated_answers.append(generated_answer)\n",
    "    reference_answers.append(sample[\"long_answer\"].strip())\n",
    "\n",
    "print(\"Generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a0c9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f479a7b798b549dc9c78d034b1516aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073f0d3c16be4c8697f69396ca6aef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 27.70 seconds, 36.10 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = bert_score(generated_answers, reference_answers, lang=\"en\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16ee207",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_matches = (F1 >= THRESHOLD).numpy()\n",
    "accuracy = np.mean(soft_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2582bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BioGPT Instruction fine tuning Text Answer Evaluation (on 1000 samples):\n",
      "1. Soft Accuracy (BERTScore F1 ≥ 85%): 0.1300\n",
      "2. BERTScore Precision: 0.7947\n",
      "3. BERTScore Recall:    0.8768\n",
      "4. BERTScore F1:        0.8336\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBioGPT Instruction fine tuning Text Answer Evaluation (on {len(dataset)} samples):\")\n",
    "print(f\"1. Soft Accuracy (BERTScore F1 ≥ {int(THRESHOLD*100)}%): {accuracy:.4f}\")\n",
    "print(f\"2. BERTScore Precision: {P.mean():.4f}\")\n",
    "print(f\"3. BERTScore Recall:    {R.mean():.4f}\")\n",
    "print(f\"4. BERTScore F1:        {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef2c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 870 wrong predictions to biogpt_wrong_answers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "questions = dataset[\"question\"]\n",
    "contexts = [\" \".join(ctx[\"contexts\"]) for ctx in dataset[\"context\"]]\n",
    "pubids = dataset[\"pubid\"]\n",
    "\n",
    "threshold = 0.85\n",
    "f1_scores = F1.numpy()\n",
    "wrong_indices = [i for i, score in enumerate(f1_scores) if score < threshold]\n",
    "\n",
    "wrong_df = pd.DataFrame({\n",
    "    \"PubMed ID\": [pubids[i] for i in wrong_indices],\n",
    "    \"Question\": [questions[i] for i in wrong_indices],\n",
    "    \"Context\": [contexts[i] for i in wrong_indices],\n",
    "    \"Generated Answer\": [generated_answers[i] for i in wrong_indices],\n",
    "    \"Gold Answer\": [reference_answers[i] for i in wrong_indices],\n",
    "    \"BERTScore F1\": [f1_scores[i] for i in wrong_indices]\n",
    "})\n",
    "\n",
    "wrong_df.to_csv(\"biogpt_lora_0.85_wrong_answers.csv\", index=False)\n",
    "print(f\"Saved {len(wrong_df)} wrong predictions to biogpt_wrong_answers.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biogpt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
